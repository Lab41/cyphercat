{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled Faces in the Wild "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "sys.path.insert(0, '../src_code')\n",
    "\n",
    "import models\n",
    "from train import *\n",
    "from metrics import *  \n",
    "\n",
    "print(\"Python: %s\" % sys.version)\n",
    "print(\"Pytorch: %s\" % torch.__version__)\n",
    "\n",
    "# determine device to run network on (runs on gpu if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 8\n",
    "lr = 0.001\n",
    "k = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Labeled Faces in the Wild \n",
    "### http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../datasets/lfw/lfw_15/\"\n",
    "\n",
    "img_paths = []\n",
    "for p in os.listdir(data_dir): \n",
    "    for i in os.listdir(os.path.join(data_dir,p)): \n",
    "        img_paths.append(os.path.join(data_dir,p,i))\n",
    "        \n",
    "people = []\n",
    "people_to_idx = {}\n",
    "k = 0 \n",
    "for i in img_paths: \n",
    "    name = i.split('/')[-2]\n",
    "    if name not in people_to_idx: \n",
    "        people.append(name)\n",
    "        people_to_idx[name] = k\n",
    "        k += 1\n",
    "\n",
    "\n",
    "img_paths = np.random.permutation(img_paths)\n",
    "\n",
    "lfw_size = len(img_paths)\n",
    "\n",
    "lfw_train_size = int(0.8 * lfw_size)\n",
    "\n",
    "lfw_train_list = img_paths[:lfw_train_size]\n",
    "lfw_test_list = img_paths[lfw_train_size:]\n",
    "\n",
    "class LFWDataset(Dataset): \n",
    "    def __init__(self, file_list, class_to_label, transform=None): \n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.people_to_idx = class_to_label\n",
    "        \n",
    "                \n",
    "    def __len__(self): \n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, idx): \n",
    "        img_path = self.file_list[idx]\n",
    "        image = io.imread(img_path)\n",
    "        label = self.people_to_idx[img_path.split('/')[-2]]\n",
    "        \n",
    "        if self.transform is not None: \n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "\n",
    "# Data augmentation \n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.RandomRotation(10),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(),\n",
    "    #torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    " \n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "    \n",
    "\n",
    "trainset = LFWDataset(lfw_train_list, people_to_idx, transform=train_transform)\n",
    "testset = LFWDataset(lfw_test_list, people_to_idx, transform=test_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# helper function to unnormalize and plot image \n",
    "def imshow(img):\n",
    "    img = np.array(img)\n",
    "    img = img / 2 + 0.5\n",
    "    img = np.moveaxis(img, 0, -1)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "# display sample from dataset \n",
    "imgs,labels = iter(trainloader).next()\n",
    "imshow(torchvision.utils.make_grid(imgs))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.tiny_cnn(n_in=3, n_out=200, n_hidden=32, size=250).to(device)\n",
    "\n",
    "net.apply(models.weights_init)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(net, trainloader, testloader, optimizer, loss, n_epochs, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
